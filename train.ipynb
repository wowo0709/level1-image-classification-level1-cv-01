{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import MaskDataset, MaskDataLoader\n",
    "\n",
    "import os\n",
    "import logging\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms, models\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image, ImageFile\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "MODEL_SAVE_PATH = '/opt/ml/checkpoints'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import error\n",
    "# %conda install -c conda-forge ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=7, stride=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.25)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(-1, 128)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "efficientnet_b0\n",
      "efficientnet_b1\n",
      "efficientnet_b1_pruned\n",
      "efficientnet_b2\n",
      "efficientnet_b2_pruned\n",
      "efficientnet_b3\n",
      "efficientnet_b3_pruned\n",
      "efficientnet_b4\n",
      "efficientnet_el\n",
      "efficientnet_el_pruned\n",
      "efficientnet_em\n",
      "efficientnet_es\n",
      "efficientnet_es_pruned\n",
      "efficientnet_lite0\n",
      "efficientnetv2_rw_m\n",
      "efficientnetv2_rw_s\n",
      "efficientnetv2_rw_t\n",
      "gc_efficientnetv2_rw_t\n",
      "swin_base_patch4_window7_224\n",
      "swin_base_patch4_window7_224_in22k\n",
      "swin_base_patch4_window12_384\n",
      "swin_base_patch4_window12_384_in22k\n",
      "swin_large_patch4_window7_224\n",
      "swin_large_patch4_window7_224_in22k\n",
      "swin_large_patch4_window12_384\n",
      "swin_large_patch4_window12_384_in22k\n",
      "swin_small_patch4_window7_224\n",
      "swin_tiny_patch4_window7_224\n",
      "tf_efficientnet_b0\n",
      "tf_efficientnet_b0_ap\n",
      "tf_efficientnet_b0_ns\n",
      "tf_efficientnet_b1\n",
      "tf_efficientnet_b1_ap\n",
      "tf_efficientnet_b1_ns\n",
      "tf_efficientnet_b2\n",
      "tf_efficientnet_b2_ap\n",
      "tf_efficientnet_b2_ns\n",
      "tf_efficientnet_b3\n",
      "tf_efficientnet_b3_ap\n",
      "tf_efficientnet_b3_ns\n",
      "tf_efficientnet_b4\n",
      "tf_efficientnet_b4_ap\n",
      "tf_efficientnet_b4_ns\n",
      "tf_efficientnet_b5\n",
      "tf_efficientnet_b5_ap\n",
      "tf_efficientnet_b5_ns\n",
      "tf_efficientnet_b6\n",
      "tf_efficientnet_b6_ap\n",
      "tf_efficientnet_b6_ns\n",
      "tf_efficientnet_b7\n",
      "tf_efficientnet_b7_ap\n",
      "tf_efficientnet_b7_ns\n",
      "tf_efficientnet_b8\n",
      "tf_efficientnet_b8_ap\n",
      "tf_efficientnet_cc_b0_4e\n",
      "tf_efficientnet_cc_b0_8e\n",
      "tf_efficientnet_cc_b1_8e\n",
      "tf_efficientnet_el\n",
      "tf_efficientnet_em\n",
      "tf_efficientnet_es\n",
      "tf_efficientnet_l2_ns\n",
      "tf_efficientnet_l2_ns_475\n",
      "tf_efficientnet_lite0\n",
      "tf_efficientnet_lite1\n",
      "tf_efficientnet_lite2\n",
      "tf_efficientnet_lite3\n",
      "tf_efficientnet_lite4\n",
      "tf_efficientnetv2_b0\n",
      "tf_efficientnetv2_b1\n",
      "tf_efficientnetv2_b2\n",
      "tf_efficientnetv2_b3\n",
      "tf_efficientnetv2_l\n",
      "tf_efficientnetv2_l_in21ft1k\n",
      "tf_efficientnetv2_l_in21k\n",
      "tf_efficientnetv2_m\n",
      "tf_efficientnetv2_m_in21ft1k\n",
      "tf_efficientnetv2_m_in21k\n",
      "tf_efficientnetv2_s\n",
      "tf_efficientnetv2_s_in21ft1k\n",
      "tf_efficientnetv2_s_in21k\n",
      "tf_efficientnetv2_xl_in21ft1k\n",
      "tf_efficientnetv2_xl_in21k\n"
     ]
    }
   ],
   "source": [
    "pretrained_list = timm.list_models(pretrained=True)\n",
    "# print(pretrained_list)\n",
    "for model in pretrained_list:\n",
    "    if 'swin' in model:\n",
    "        print(model)\n",
    "    if 'efficient' in model:\n",
    "        print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torchvision.models' has no attribute 'efficientnet_b3'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/opt/ml/code/train.ipynb Cell 5'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B27.96.130.197/opt/ml/code/train.ipynb#ch0000004vscode-remote?line=0'>1</a>\u001b[0m \u001b[39m# target_model = models.resnext50_32x4d(pretrained=True)\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B27.96.130.197/opt/ml/code/train.ipynb#ch0000004vscode-remote?line=1'>2</a>\u001b[0m target_model \u001b[39m=\u001b[39m models\u001b[39m.\u001b[39;49mefficientnet_b3(pretrianed\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B27.96.130.197/opt/ml/code/train.ipynb#ch0000004vscode-remote?line=2'>3</a>\u001b[0m \u001b[39m# target_model = timm.create_model('efficientnet_b3', pretrained=True, num_classes=18)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B27.96.130.197/opt/ml/code/train.ipynb#ch0000004vscode-remote?line=3'>4</a>\u001b[0m \u001b[39m# target_model = BaseModel(18)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B27.96.130.197/opt/ml/code/train.ipynb#ch0000004vscode-remote?line=4'>5</a>\u001b[0m model_name \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mefficientnet_b3\u001b[39m\u001b[39m'\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torchvision.models' has no attribute 'efficientnet_b3'"
     ]
    }
   ],
   "source": [
    "# target_model = models.resnext50_32x4d(pretrained=True)\n",
    "target_model = models.efficientnet_b3(pretrianed=True)\n",
    "# target_model = timm.create_model('efficientnet_b3', pretrained=True, num_classes=18)\n",
    "# target_model = BaseModel(18)\n",
    "model_name = 'efficientnet_b3'\n",
    "\n",
    "# print(\"네트워크 필요 입력 채널 개수\", target_model.layers[0].weight.shape[1])\n",
    "# print(\"네트워크 출력 채널 개수 (예측 class type 개수)\", target_model.layers[-1].weight.shape[0])\n",
    "\n",
    "print(target_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/ml/code\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Converting train dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b99c48e2588e467189e3660c3348ad62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "18900it: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Converting test dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dc1d43dbc5b4cc7b4c664087c6eca3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "12600it: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean = (0.548, 0.504, 0.479)\n",
    "std = (0.237, 0.247, 0.246)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "                    # transforms.Resize((384,384)), \n",
    "                    transforms.RandomCrop((384,384)),\n",
    "                    transforms.ToTensor(), \n",
    "                    transforms.Normalize(mean, std)\n",
    "                ])\n",
    "train_dataset = MaskDataset(transform, training=True)\n",
    "test_dataset = MaskDataset(transform, training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[ 0.6827,  0.6993,  0.6993,  ..., -1.5842, -1.4684, -1.3360],\n",
      "         [ 0.6827,  0.6993,  0.6993,  ..., -1.5676, -1.4022, -1.2367],\n",
      "         [ 0.6827,  0.6827,  0.6827,  ..., -1.8158, -1.7993, -1.6007],\n",
      "         ...,\n",
      "         [ 0.4842,  0.5007,  0.5007,  ...,  0.5338,  0.4180,  0.3683],\n",
      "         [ 0.4842,  0.4842,  0.4842,  ...,  0.5173,  0.4511,  0.3849],\n",
      "         [ 0.4676,  0.4676,  0.4676,  ...,  0.5338,  0.4676,  0.4014]],\n",
      "\n",
      "        [[ 0.8015,  0.8173,  0.8173,  ..., -1.8817, -1.8817, -1.7706],\n",
      "         [ 0.8015,  0.8173,  0.8173,  ..., -1.8182, -1.7388, -1.6277],\n",
      "         [ 0.8015,  0.8015,  0.8015,  ..., -1.9452, -1.9452, -1.8023],\n",
      "         ...,\n",
      "         [-0.1829, -0.1670, -0.1670,  ..., -0.2147, -0.2623, -0.3099],\n",
      "         [-0.1829, -0.1829, -0.1829,  ..., -0.1829, -0.2305, -0.2623],\n",
      "         [-0.1988, -0.1988, -0.1988,  ..., -0.1670, -0.2147, -0.2464]],\n",
      "\n",
      "        [[ 0.7150,  0.7310,  0.7310,  ..., -1.7718, -1.7240, -1.6124],\n",
      "         [ 0.7150,  0.7310,  0.7310,  ..., -1.7559, -1.6124, -1.4849],\n",
      "         [ 0.7150,  0.7150,  0.7150,  ..., -1.9153, -1.9153, -1.7559],\n",
      "         ...,\n",
      "         [-0.7197, -0.7037, -0.7037,  ..., -0.7994, -0.8631, -0.8791],\n",
      "         [-0.7197, -0.7197, -0.7197,  ..., -0.7834, -0.7994, -0.8472],\n",
      "         [-0.7356, -0.7356, -0.7356,  ..., -0.7675, -0.7834, -0.8313]]]), 4)\n",
      "(tensor([[[-1.1705, -1.6338, -1.8655,  ..., -1.1209, -1.2202, -1.5180],\n",
      "         [-1.6007, -1.6835, -1.6504,  ..., -1.2202, -1.3360, -1.5676],\n",
      "         [-1.6173, -1.6007, -1.6504,  ..., -1.5014, -1.5842, -1.6173],\n",
      "         ...,\n",
      "         [-1.2202, -1.1871, -1.1540,  ..., -1.2367, -1.4022, -1.3856],\n",
      "         [-1.2367, -1.1871, -1.1540,  ..., -1.4684, -1.4353, -1.5180],\n",
      "         [-1.2202, -1.2036, -1.1705,  ..., -1.4518, -1.3691, -1.4187]],\n",
      "\n",
      "        [[-0.8497, -1.3102, -1.5324,  ..., -0.8815, -0.9767, -1.2625],\n",
      "         [-1.2625, -1.3578, -1.3260,  ..., -0.9767, -1.0879, -1.3102],\n",
      "         [-1.2784, -1.2784, -1.3260,  ..., -1.2466, -1.3260, -1.3578],\n",
      "         ...,\n",
      "         [-0.9132, -0.8815, -0.8497,  ..., -0.9609, -1.1196, -1.1038],\n",
      "         [-0.8974, -0.8497, -0.8180,  ..., -1.1831, -1.1514, -1.2308],\n",
      "         [-0.8815, -0.8656, -0.8338,  ..., -1.1673, -1.0879, -1.1355]],\n",
      "\n",
      "        [[-0.9110, -1.3254, -1.5486,  ..., -0.9110, -1.0066, -1.2936],\n",
      "         [-1.3254, -1.3733, -1.3414,  ..., -1.0066, -1.1182, -1.3414],\n",
      "         [-1.3414, -1.2936, -1.3414,  ..., -1.2776, -1.3573, -1.3892],\n",
      "         ...,\n",
      "         [-0.2095, -0.1777, -0.1458,  ..., -0.5921, -0.7516, -0.7356],\n",
      "         [-0.3371, -0.2893, -0.2574,  ..., -0.7834, -0.7516, -0.8313],\n",
      "         [-0.3211, -0.3052, -0.2733,  ..., -0.7675, -0.6878, -0.7356]]]), 0)\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0])\n",
    "print(test_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 384, 384])\n",
      "torch.Size([3, 384, 384])\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0][0].size())\n",
    "print(test_dataset[0][0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMAGE_INDEX = 0\n",
    "# fig, axes = plt.subplots(1, 2, figsize=(15,7))\n",
    "\n",
    "# img, label = train_dataset[IMAGE_INDEX][0], train_dataset[IMAGE_INDEX][1]\n",
    "# normalized_img = to_pil_image(img)\n",
    "# origin_img = to_pil_image(std*img+mean)\n",
    "\n",
    "# axes[0].imshow(normalized_img)\n",
    "# axes[0].set_title('Normalized image')\n",
    "# axes[0].axis('off')\n",
    "# axes[1].imshow(origin_img)\n",
    "# axes[1].set_title('Origin image')\n",
    "# axes[1].axis('off')\n",
    "\n",
    "# print(f'label: ', label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "BATCH_SIZE = 64\n",
    "train_dataloader = MaskDataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False, validation_split=0.2)\n",
    "val_dataloader = train_dataloader.split_validation()\n",
    "# test_dataloader = MaskDataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[[ 1.1460,  1.1460,  1.1460,  ...,  1.0137,  1.0137,  0.9971],\n",
       "           [ 1.1460,  1.1460,  1.1460,  ...,  1.0137,  0.9971,  0.9971],\n",
       "           [ 1.1460,  1.1460,  1.1460,  ...,  0.9971,  0.9971,  0.9971],\n",
       "           ...,\n",
       "           [ 1.3611,  1.3611,  1.3115,  ..., -0.3432, -0.3432, -0.3266],\n",
       "           [ 1.3611,  1.3611,  1.3115,  ..., -0.3597, -0.3432, -0.3266],\n",
       "           [ 1.3611,  1.3611,  1.3115,  ..., -0.3597, -0.3597, -0.3266]],\n",
       " \n",
       "          [[ 1.2778,  1.2778,  1.2778,  ...,  1.0872,  1.0872,  1.0714],\n",
       "           [ 1.2778,  1.2778,  1.2778,  ...,  1.0872,  1.0714,  1.0714],\n",
       "           [ 1.2778,  1.2778,  1.2778,  ...,  1.0714,  1.0714,  1.0714],\n",
       "           ...,\n",
       "           [ 1.5000,  1.5000,  1.4524,  ..., -0.1829, -0.1829, -0.1670],\n",
       "           [ 1.5000,  1.5000,  1.4524,  ..., -0.1988, -0.1829, -0.1670],\n",
       "           [ 1.5000,  1.5000,  1.4524,  ..., -0.1988, -0.1988, -0.1670]],\n",
       " \n",
       "          [[ 1.2571,  1.2571,  1.2571,  ...,  1.0020,  1.0020,  0.9861],\n",
       "           [ 1.2571,  1.2571,  1.2571,  ...,  1.0020,  0.9861,  0.9861],\n",
       "           [ 1.2571,  1.2571,  1.2571,  ...,  0.9861,  0.9861,  0.9861],\n",
       "           ...,\n",
       "           [ 1.4802,  1.4802,  1.4324,  ..., -0.2733, -0.2733, -0.2574],\n",
       "           [ 1.4802,  1.4802,  1.4324,  ..., -0.2893, -0.2733, -0.2574],\n",
       "           [ 1.4802,  1.4802,  1.4324,  ..., -0.2893, -0.2893, -0.2574]]],\n",
       " \n",
       " \n",
       "         [[[-1.1374, -1.0547, -1.0050,  ..., -0.4756, -0.4921, -0.4921],\n",
       "           [-1.0712, -1.0547, -1.0712,  ..., -0.4756, -0.4756, -0.4756],\n",
       "           [-1.0712, -1.1043, -1.1374,  ..., -0.4756, -0.4756, -0.4756],\n",
       "           ...,\n",
       "           [-0.6410, -0.4259, -0.3928,  ...,  0.3849,  0.3849,  0.3849],\n",
       "           [-0.6741, -0.4756, -0.4259,  ...,  0.3849,  0.3849,  0.3849],\n",
       "           [-0.6741, -0.4590, -0.3763,  ...,  0.3849,  0.3849,  0.3849]],\n",
       " \n",
       "          [[-1.0720, -0.9767, -0.9291,  ..., -0.3575, -0.3734, -0.3734],\n",
       "           [-1.0085, -0.9767, -0.9926,  ..., -0.3575, -0.3575, -0.3575],\n",
       "           [-1.0085, -1.0244, -1.0561,  ..., -0.3575, -0.3575, -0.3575],\n",
       "           ...,\n",
       "           [-0.5639, -0.3575, -0.3258,  ...,  0.4998,  0.4998,  0.4998],\n",
       "           [-0.5957, -0.4052, -0.3575,  ...,  0.4998,  0.4998,  0.4998],\n",
       "           [-0.5957, -0.3893, -0.3099,  ...,  0.4998,  0.4998,  0.4998]],\n",
       " \n",
       "          [[-1.4051, -1.3414, -1.2936,  ..., -0.3530, -0.3690, -0.3690],\n",
       "           [-1.3414, -1.3414, -1.3573,  ..., -0.3530, -0.3530, -0.3530],\n",
       "           [-1.3414, -1.3892, -1.4211,  ..., -0.3530, -0.3530, -0.3530],\n",
       "           ...,\n",
       "           [-0.6400, -0.4327, -0.4008,  ...,  0.5238,  0.5238,  0.5238],\n",
       "           [-0.6718, -0.4806, -0.4327,  ...,  0.5238,  0.5238,  0.5238],\n",
       "           [-0.6718, -0.4646, -0.3849,  ...,  0.5238,  0.5238,  0.5238]]],\n",
       " \n",
       " \n",
       "         [[[ 0.7489,  0.7489,  0.7489,  ...,  1.0467,  1.0467,  1.0467],\n",
       "           [ 0.7489,  0.7489,  0.7489,  ...,  1.0467,  1.0467,  1.0467],\n",
       "           [ 0.7489,  0.7489,  0.7489,  ...,  1.0467,  1.0467,  1.0467],\n",
       "           ...,\n",
       "           [ 0.6662,  0.6662,  0.7324,  ...,  1.1957,  1.1791,  1.1626],\n",
       "           [ 0.6496,  0.6662,  0.7489,  ...,  1.1957,  1.1791,  1.1626],\n",
       "           [ 0.6000,  0.6496,  0.7489,  ...,  1.1957,  1.1791,  1.1626]],\n",
       " \n",
       "          [[ 0.8173,  0.8173,  0.8173,  ...,  1.1349,  1.1349,  1.1349],\n",
       "           [ 0.8173,  0.8173,  0.8173,  ...,  1.1349,  1.1349,  1.1349],\n",
       "           [ 0.8173,  0.8173,  0.8173,  ...,  1.1349,  1.1349,  1.1349],\n",
       "           ...,\n",
       "           [ 0.7380,  0.7380,  0.8015,  ...,  1.2301,  1.2143,  1.1984],\n",
       "           [ 0.7221,  0.7380,  0.8173,  ...,  1.2301,  1.2143,  1.1984],\n",
       "           [ 0.6744,  0.7221,  0.8173,  ...,  1.2301,  1.2143,  1.1984]],\n",
       " \n",
       "          [[ 0.8266,  0.8266,  0.8266,  ...,  1.0020,  1.0020,  1.0020],\n",
       "           [ 0.8266,  0.8266,  0.8266,  ...,  1.0020,  1.0020,  1.0020],\n",
       "           [ 0.8266,  0.8266,  0.8266,  ...,  1.0020,  1.0020,  1.0020],\n",
       "           ...,\n",
       "           [ 0.7948,  0.7948,  0.8585,  ...,  1.5281,  1.5121,  1.4962],\n",
       "           [ 0.7788,  0.7948,  0.8745,  ...,  1.5281,  1.5121,  1.4962],\n",
       "           [ 0.7310,  0.7788,  0.8745,  ...,  1.5281,  1.5121,  1.4962]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[ 1.1295,  1.1295,  1.1295,  ...,  1.1295,  1.1295,  1.1295],\n",
       "           [ 1.1295,  1.1295,  1.1295,  ...,  1.1295,  1.1295,  1.1295],\n",
       "           [ 1.1295,  1.1295,  1.1295,  ...,  1.1295,  1.1295,  1.1295],\n",
       "           ...,\n",
       "           [-1.0712, -1.0712, -1.0712,  ..., -0.9720, -1.2863, -1.4187],\n",
       "           [-1.1209, -1.1209, -1.1043,  ..., -0.9720, -1.2698, -1.4187],\n",
       "           [-1.1705, -1.1705, -1.1540,  ..., -0.9554, -1.2532, -1.4187]],\n",
       " \n",
       "          [[ 1.2143,  1.2143,  1.2143,  ...,  1.2143,  1.2143,  1.2143],\n",
       "           [ 1.2143,  1.2143,  1.2143,  ...,  1.2143,  1.2143,  1.2143],\n",
       "           [ 1.2143,  1.2143,  1.2143,  ...,  1.2143,  1.2143,  1.2143],\n",
       "           ...,\n",
       "           [-0.7862, -0.7862, -0.7862,  ..., -0.6910, -0.9926, -1.1196],\n",
       "           [-0.8338, -0.8338, -0.8180,  ..., -0.6910, -0.9767, -1.1196],\n",
       "           [-0.8815, -0.8815, -0.8656,  ..., -0.6751, -0.9609, -1.1196]],\n",
       " \n",
       "          [[ 1.2411,  1.2411,  1.2411,  ...,  1.2411,  1.2411,  1.2411],\n",
       "           [ 1.2411,  1.2411,  1.2411,  ...,  1.2411,  1.2411,  1.2411],\n",
       "           [ 1.2411,  1.2411,  1.2411,  ...,  1.2411,  1.2411,  1.2411],\n",
       "           ...,\n",
       "           [-0.6718, -0.6718, -0.6718,  ..., -0.5443, -0.8472, -0.9747],\n",
       "           [-0.7197, -0.7197, -0.7037,  ..., -0.5443, -0.8313, -0.9747],\n",
       "           [-0.7675, -0.7675, -0.7516,  ..., -0.5284, -0.8153, -0.9747]]],\n",
       " \n",
       " \n",
       "         [[[-1.0216, -1.0381, -1.0547,  ..., -0.4425, -0.4425, -0.4425],\n",
       "           [-1.0216, -1.0216, -1.0381,  ..., -0.4425, -0.4425, -0.4425],\n",
       "           [-1.0050, -1.0216, -1.0381,  ..., -0.4425, -0.4425, -0.4425],\n",
       "           ...,\n",
       "           [-1.6835, -1.6835, -1.6835,  ..., -1.5014, -1.4353, -1.4022],\n",
       "           [-1.6835, -1.6835, -1.6835,  ..., -1.5180, -1.4849, -1.4518],\n",
       "           [-1.6835, -1.6835, -1.6835,  ..., -1.5511, -1.5345, -1.5014]],\n",
       " \n",
       "          [[-0.8021, -0.8180, -0.8338,  ..., -0.3099, -0.3099, -0.3099],\n",
       "           [-0.8021, -0.8021, -0.8180,  ..., -0.3099, -0.3099, -0.3099],\n",
       "           [-0.7862, -0.8021, -0.8180,  ..., -0.3099, -0.3099, -0.3099],\n",
       "           ...,\n",
       "           [-1.2308, -1.2308, -1.2308,  ..., -1.0879, -1.0244, -0.9926],\n",
       "           [-1.2308, -1.2308, -1.2308,  ..., -1.1038, -1.0720, -1.0402],\n",
       "           [-1.2308, -1.2308, -1.2308,  ..., -1.1355, -1.1196, -1.0879]],\n",
       " \n",
       "          [[-0.7037, -0.7197, -0.7356,  ..., -0.2255, -0.2255, -0.2255],\n",
       "           [-0.7037, -0.7037, -0.7197,  ..., -0.2255, -0.2255, -0.2255],\n",
       "           [-0.6878, -0.7037, -0.7197,  ..., -0.2255, -0.2255, -0.2255],\n",
       "           ...,\n",
       "           [-1.2936, -1.2936, -1.2936,  ..., -1.0863, -1.0544, -1.0226],\n",
       "           [-1.2936, -1.2936, -1.2936,  ..., -1.1023, -1.1023, -1.0704],\n",
       "           [-1.2936, -1.2936, -1.2936,  ..., -1.1341, -1.1501, -1.1182]]],\n",
       " \n",
       " \n",
       "         [[[ 0.6993,  0.6993,  0.7158,  ...,  0.1036, -0.1115, -0.2935],\n",
       "           [ 0.9806,  0.9640,  0.9475,  ..., -0.0950, -0.3266, -0.5086],\n",
       "           [ 0.8482,  0.8151,  0.7655,  ..., -0.2935, -0.4921, -0.6245],\n",
       "           ...,\n",
       "           [-0.1115, -0.1115, -0.1115,  ..., -0.4590, -0.4756, -0.4921],\n",
       "           [-0.1115, -0.1115, -0.1115,  ..., -0.4756, -0.5086, -0.5086],\n",
       "           [-0.0950, -0.0950, -0.0950,  ..., -0.5252, -0.5252, -0.5252]],\n",
       " \n",
       "          [[ 0.6744,  0.6744,  0.6903,  ..., -0.5639, -0.7068, -0.8497],\n",
       "           [ 0.6427,  0.6268,  0.6109,  ..., -0.7386, -0.8815, -1.0402],\n",
       "           [ 0.1664,  0.1346,  0.0870,  ..., -0.9132, -1.0402, -1.1355],\n",
       "           ...,\n",
       "           [ 0.0394,  0.0394,  0.0394,  ..., -0.4211, -0.4369, -0.4369],\n",
       "           [ 0.0394,  0.0394,  0.0394,  ..., -0.4369, -0.4687, -0.4528],\n",
       "           [ 0.0394,  0.0394,  0.0394,  ..., -0.4846, -0.5004, -0.5004]],\n",
       " \n",
       "          [[ 0.7150,  0.7150,  0.7310,  ..., -0.6878, -0.8472, -1.0066],\n",
       "           [ 0.6194,  0.6035,  0.5875,  ..., -0.8631, -1.0385, -1.1979],\n",
       "           [ 0.0615,  0.0296, -0.0183,  ..., -1.0863, -1.2298, -1.3414],\n",
       "           ...,\n",
       "           [-0.0661, -0.0661, -0.0661,  ..., -0.4806, -0.5284, -0.5762],\n",
       "           [-0.0661, -0.0661, -0.0661,  ..., -0.4965, -0.5603, -0.5921],\n",
       "           [-0.0980, -0.0980, -0.0980,  ..., -0.5284, -0.4965, -0.4646]]]]),\n",
       " tensor([ 3,  4,  3,  4,  1,  4,  3,  0, 12,  0,  3,  3,  0, 16,  4,  3,  4,  1,\n",
       "          9,  6, 13,  4,  3,  1,  0,  4, 16,  3,  1, 17,  4,  4,  6,  3,  3,  4,\n",
       "         10, 15,  4, 13,  1,  0,  1,  9,  6,  6,  7,  0,  4,  4,  0,  2,  4,  1,\n",
       "          4,  5,  4,  6,  5,  3, 15,  7,  3,  1])]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change last FC layer\n",
    "# import math\n",
    "\n",
    "# MASK_CLASS_NUM = 18\n",
    "# in_features = target_model.fc.in_features\n",
    "# target_model.fc = torch.nn.Linear(in_features=in_features, out_features=MASK_CLASS_NUM, bias=True)\n",
    "\n",
    "# torch.nn.init.xavier_uniform_(target_model.fc.weight)\n",
    "# stdv = 1. / math.sqrt(target_model.fc.weight.size(1))\n",
    "# target_model.fc.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "# print(\"네트워크 필요 입력 채널 개수\", target_model.conv1.weight.shape[1])\n",
    "# print(\"네트워크 출력 채널 개수 (예측 class type 개수)\", target_model.fc.weight.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0 is using!\n"
     ]
    }
   ],
   "source": [
    "# Compile options\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"{device} is using!\")\n",
    "\n",
    "# resnext50.to(device)\n",
    "\n",
    "LEARNING_RATE = 0.0001\n",
    "NUM_EPOCH = 10\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(target_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "dataloaders = {\n",
    "    \"train\": train_dataloader, \n",
    "    \"val\": val_dataloader\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f86d478ce20d4d91a152ff970f6371af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/237 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-0의 train-데이터 셋에서 평균 Loss : 0.007, 평균 Accuracy : 0.696\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1dd7eef04fd4e2c8274805e4002b8d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-0의 val-데이터 셋에서 평균 Loss : 0.000, 평균 Accuracy : 0.190\n",
      "New best model for val accuracy! Saving the model...\n",
      "New best model for val loss! Saving the model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acf379b08d2b49b197d556256a49b62a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/237 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-1의 train-데이터 셋에서 평균 Loss : 0.021, 평균 Accuracy : 0.397\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "687ede17f5e54e5c869f2a10bf259227",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-1의 val-데이터 셋에서 평균 Loss : 0.005, 평균 Accuracy : 0.104\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac90ded8f6c44d02872d818442a70441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/237 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-2의 train-데이터 셋에서 평균 Loss : 0.017, 평균 Accuracy : 0.440\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f27e279b72784eccb928db2c6e358c73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-2의 val-데이터 셋에서 평균 Loss : 0.005, 평균 Accuracy : 0.099\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e108520e40bf4a7f8761cfd4f762c545",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/237 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-3의 train-데이터 셋에서 평균 Loss : 0.016, 평균 Accuracy : 0.462\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fe237f526a04cb4807d1944db3f260c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-3의 val-데이터 셋에서 평균 Loss : 0.004, 평균 Accuracy : 0.110\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f99d1a9fb93e4b9daac6317d34e33c5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/237 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-4의 train-데이터 셋에서 평균 Loss : 0.015, 평균 Accuracy : 0.476\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ecdd9326bc54894a20356c8a55496e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-4의 val-데이터 셋에서 평균 Loss : 0.004, 평균 Accuracy : 0.111\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "721446cecfc748869e9733e3456fc204",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/237 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-5의 train-데이터 셋에서 평균 Loss : 0.015, 평균 Accuracy : 0.487\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2464cb18362b4c04aabb1556fad88fe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-5의 val-데이터 셋에서 평균 Loss : 0.004, 평균 Accuracy : 0.114\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c244de3a44e4350b04e86064b5bedc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/237 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-6의 train-데이터 셋에서 평균 Loss : 0.014, 평균 Accuracy : 0.499\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd98914d268f402d897811b41f8c4dc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-6의 val-데이터 셋에서 평균 Loss : 0.004, 평균 Accuracy : 0.113\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44e8ab630f6546158eaf5f76d2cda0fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/237 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-7의 train-데이터 셋에서 평균 Loss : 0.013, 평균 Accuracy : 0.511\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "561d52baebc549bc9a64b00968fd9f2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-7의 val-데이터 셋에서 평균 Loss : 0.004, 평균 Accuracy : 0.114\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe3e6669b66344929be8df2898cddb4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/237 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m   \u001b[39mif\u001b[39;00m phase \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     36\u001b[0m     loss\u001b[39m.\u001b[39mbackward() \u001b[39m# 모델의 예측 값과 실제 값의 CrossEntropy 차이를 통해 gradient 계산\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep() \u001b[39m# 계산된 gradient를 가지고 모델 업데이트\u001b[39;00m\n\u001b[1;32m     39\u001b[0m running_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem() \u001b[39m#* images.size(0) # 한 Batch에서의 loss 값 저장\u001b[39;00m\n\u001b[1;32m     40\u001b[0m running_acc \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msum(preds \u001b[39m==\u001b[39m labels\u001b[39m.\u001b[39mdata) \u001b[39m# 한 Batch에서의 Accuracy 값 저장\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/ImageClassification/lib/python3.8/site-packages/torch/autograd/grad_mode.py:26\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///opt/conda/envs/ImageClassification/lib/python3.8/site-packages/torch/autograd/grad_mode.py?line=22'>23</a>\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     <a href='file:///opt/conda/envs/ImageClassification/lib/python3.8/site-packages/torch/autograd/grad_mode.py?line=23'>24</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     <a href='file:///opt/conda/envs/ImageClassification/lib/python3.8/site-packages/torch/autograd/grad_mode.py?line=24'>25</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m():\n\u001b[0;32m---> <a href='file:///opt/conda/envs/ImageClassification/lib/python3.8/site-packages/torch/autograd/grad_mode.py?line=25'>26</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/ImageClassification/lib/python3.8/site-packages/torch/optim/adam.py:108\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/conda/envs/ImageClassification/lib/python3.8/site-packages/torch/optim/adam.py?line=104'>105</a>\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m    <a href='file:///opt/conda/envs/ImageClassification/lib/python3.8/site-packages/torch/optim/adam.py?line=106'>107</a>\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m'\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m--> <a href='file:///opt/conda/envs/ImageClassification/lib/python3.8/site-packages/torch/optim/adam.py?line=107'>108</a>\u001b[0m     F\u001b[39m.\u001b[39;49madam(params_with_grad,\n\u001b[1;32m    <a href='file:///opt/conda/envs/ImageClassification/lib/python3.8/site-packages/torch/optim/adam.py?line=108'>109</a>\u001b[0m            grads,\n\u001b[1;32m    <a href='file:///opt/conda/envs/ImageClassification/lib/python3.8/site-packages/torch/optim/adam.py?line=109'>110</a>\u001b[0m            exp_avgs,\n\u001b[1;32m    <a href='file:///opt/conda/envs/ImageClassification/lib/python3.8/site-packages/torch/optim/adam.py?line=110'>111</a>\u001b[0m            exp_avg_sqs,\n\u001b[1;32m    <a href='file:///opt/conda/envs/ImageClassification/lib/python3.8/site-packages/torch/optim/adam.py?line=111'>112</a>\u001b[0m            max_exp_avg_sqs,\n\u001b[1;32m    <a href='file:///opt/conda/envs/ImageClassification/lib/python3.8/site-packages/torch/optim/adam.py?line=112'>113</a>\u001b[0m            state_steps,\n\u001b[1;32m    <a href='file:///opt/conda/envs/ImageClassification/lib/python3.8/site-packages/torch/optim/adam.py?line=113'>114</a>\u001b[0m            group[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    <a href='file:///opt/conda/envs/ImageClassification/lib/python3.8/site-packages/torch/optim/adam.py?line=114'>115</a>\u001b[0m            beta1,\n\u001b[1;32m    <a href='file:///opt/conda/envs/ImageClassification/lib/python3.8/site-packages/torch/optim/adam.py?line=115'>116</a>\u001b[0m            beta2,\n\u001b[1;32m    <a href='file:///opt/conda/envs/ImageClassification/lib/python3.8/site-packages/torch/optim/adam.py?line=116'>117</a>\u001b[0m            group[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    <a href='file:///opt/conda/envs/ImageClassification/lib/python3.8/site-packages/torch/optim/adam.py?line=117'>118</a>\u001b[0m            group[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    <a href='file:///opt/conda/envs/ImageClassification/lib/python3.8/site-packages/torch/optim/adam.py?line=118'>119</a>\u001b[0m            group[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[1;32m    <a href='file:///opt/conda/envs/ImageClassification/lib/python3.8/site-packages/torch/optim/adam.py?line=119'>120</a>\u001b[0m            )\n\u001b[1;32m    <a href='file:///opt/conda/envs/ImageClassification/lib/python3.8/site-packages/torch/optim/adam.py?line=120'>121</a>\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/opt/conda/envs/ImageClassification/lib/python3.8/site-packages/torch/optim/functional.py:87\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     <a href='file:///opt/conda/envs/ImageClassification/lib/python3.8/site-packages/torch/optim/functional.py?line=84'>85</a>\u001b[0m \u001b[39m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m     <a href='file:///opt/conda/envs/ImageClassification/lib/python3.8/site-packages/torch/optim/functional.py?line=85'>86</a>\u001b[0m exp_avg\u001b[39m.\u001b[39mmul_(beta1)\u001b[39m.\u001b[39madd_(grad, alpha\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta1)\n\u001b[0;32m---> <a href='file:///opt/conda/envs/ImageClassification/lib/python3.8/site-packages/torch/optim/functional.py?line=86'>87</a>\u001b[0m exp_avg_sq\u001b[39m.\u001b[39;49mmul_(beta2)\u001b[39m.\u001b[39maddcmul_(grad, grad, value\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta2)\n\u001b[1;32m     <a href='file:///opt/conda/envs/ImageClassification/lib/python3.8/site-packages/torch/optim/functional.py?line=87'>88</a>\u001b[0m \u001b[39mif\u001b[39;00m amsgrad:\n\u001b[1;32m     <a href='file:///opt/conda/envs/ImageClassification/lib/python3.8/site-packages/torch/optim/functional.py?line=88'>89</a>\u001b[0m     \u001b[39m# Maintains the maximum of all 2nd moment running avg. till now\u001b[39;00m\n\u001b[1;32m     <a href='file:///opt/conda/envs/ImageClassification/lib/python3.8/site-packages/torch/optim/functional.py?line=89'>90</a>\u001b[0m     torch\u001b[39m.\u001b[39mmaximum(max_exp_avg_sq, exp_avg_sq, out\u001b[39m=\u001b[39mmax_exp_avg_sq)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training\n",
    "target_model.to(device)\n",
    "# 저장 경로 생성\n",
    "if not os.path.exists(f\"{MODEL_SAVE_PATH}/{model_name}\"):\n",
    "    os.mkdir(f\"{MODEL_SAVE_PATH}/{model_name}\")\n",
    "# freeze layers except last FC layer\n",
    "# for param in target_model.parameters()[:-1]:\n",
    "#     param.requires_grad = False\n",
    "\n",
    "best_val_accuracy = 0.\n",
    "best_val_loss = 999.\n",
    "\n",
    "for epoch in range(NUM_EPOCH):\n",
    "  for phase in [\"train\", \"val\"]:\n",
    "    running_loss = 0.\n",
    "    running_acc = 0.\n",
    "    if phase == \"train\":\n",
    "      target_model.train() # 네트워크 모델을 train 모드로 두어 gradient을 계산하고, 여러 sub module (배치 정규화, 드롭아웃 등)이 train mode로 작동할 수 있도록 함\n",
    "    elif phase == \"val\":\n",
    "      target_model.eval() # 네트워크 모델을 eval 모드 두어 여러 sub module들이 eval mode로 작동할 수 있게 함\n",
    "\n",
    "    for ind, (images, labels) in enumerate(tqdm(dataloaders[phase])):\n",
    "      # (참고.해보기) 현재 tqdm으로 출력되는 것이 단순히 진행 상황 뿐인데 현재 epoch, running_loss와 running_acc을 출력하려면 어떻게 할 수 있는지 tqdm 문서를 보고 해봅시다!\n",
    "      # hint - with, pbar\n",
    "      images = images.to(device)\n",
    "      labels = labels.to(device)\n",
    "\n",
    "      optimizer.zero_grad() # parameter gradient를 업데이트 전 초기화함\n",
    "\n",
    "      with torch.set_grad_enabled(phase == \"train\"): # train 모드일 시에는 gradient를 계산하고, 아닐 때는 gradient를 계산하지 않아 연산량 최소화\n",
    "        logits = target_model(images)\n",
    "        _, preds = torch.max(logits, 1) # 모델에서 linear 값으로 나오는 예측 값 ([0.9,1.2, 3.2,0.1,-0.1,...])을 최대 output index를 찾아 예측 레이블([2])로 변경함  \n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        if phase == \"train\":\n",
    "          loss.backward() # 모델의 예측 값과 실제 값의 CrossEntropy 차이를 통해 gradient 계산\n",
    "          optimizer.step() # 계산된 gradient를 가지고 모델 업데이트\n",
    "\n",
    "      running_loss += loss.item() #* images.size(0) # 한 Batch에서의 loss 값 저장\n",
    "      running_acc += torch.sum(preds == labels.data) # 한 Batch에서의 Accuracy 값 저장\n",
    "\n",
    "    # 한 epoch이 모두 종료되었을 때,\n",
    "    if phase == 'train':\n",
    "        epoch_loss = running_loss / (len(dataloaders[phase].dataset))\n",
    "        epoch_acc = running_acc / (len(dataloaders[phase].dataset))\n",
    "    else:\n",
    "        epoch_loss = running_loss / (len(dataloaders[phase].dataset))\n",
    "        epoch_acc = running_acc / (len(dataloaders[phase].dataset))\n",
    "\n",
    "    print(f\"현재 epoch-{epoch}의 {phase}-데이터 셋에서 평균 Loss : {epoch_loss:.3f}, 평균 Accuracy : {epoch_acc:.3f}\")\n",
    "    if phase == \"val\" and best_val_accuracy < epoch_acc: # phase가 val일 때, best accuracy 계산\n",
    "      print(\"New best model for val accuracy! Saving the model...\")\n",
    "      path = os.path.join(MODEL_SAVE_PATH, f\"{model_name}/{epoch:03}_acc_{epoch_acc:4.2%}.ckpt\")\n",
    "      torch.save(target_model.state_dict(), path)\n",
    "      best_val_accuracy = epoch_acc\n",
    "    if phase == \"val\" and best_val_loss > epoch_loss: # phase가 val일 때, best loss 계산\n",
    "      print(\"New best model for val loss! Saving the model...\")\n",
    "      path = os.path.join(MODEL_SAVE_PATH, f\"{model_name}/{epoch:03}_loss_{epoch_loss:4.2}.ckpt\")\n",
    "      torch.save(target_model.state_dict(), path)\n",
    "      best_val_loss = epoch_loss\n",
    "\n",
    "\n",
    "\n",
    "print(\"학습 종료!\")\n",
    "print(f\"최고 accuracy : {best_val_accuracy}, 최고 낮은 loss : {best_val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 모델 저장\n",
    "# PATH = os.path.join('/opt/ml/checkpoints', 'resnext50_epochs10_acc32.pt')\n",
    "# torch.save(target_model, PATH)\n",
    "# # model = torch.load(PATH)\n",
    "# # model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1646311680.461598\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "print(time.time())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class 별 개수 세기\n",
    "\n",
    "![](https://s3-us-west-2.amazonaws.com/aistages-prod-server-public/app/Users/00001204/files/d7f11b5e-2752-4aff-9d0f-6f46da8a3b19..png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/opt/ml/code/train.ipynb Cell 19'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B27.96.130.197/opt/ml/code/train.ipynb#ch0000018vscode-remote?line=54'>55</a>\u001b[0m     new_train_df[\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m new_train_df[\u001b[39m'\u001b[39m\u001b[39mmask\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m*\u001b[39m \u001b[39m6\u001b[39m \u001b[39m+\u001b[39m new_train_df[\u001b[39m'\u001b[39m\u001b[39mgender\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m*\u001b[39m\u001b[39m3\u001b[39m \u001b[39m+\u001b[39m new_train_df[\u001b[39m'\u001b[39m\u001b[39mage_group\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B27.96.130.197/opt/ml/code/train.ipynb#ch0000018vscode-remote?line=55'>56</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m new_train_df\u001b[39m.\u001b[39msort_values(by\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mreset_index(drop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B27.96.130.197/opt/ml/code/train.ipynb#ch0000018vscode-remote?line=57'>58</a>\u001b[0m new_train_df \u001b[39m=\u001b[39m preprocess_train_dataframe(DATA_PATH)\n",
      "\u001b[1;32m/opt/ml/code/train.ipynb Cell 19'\u001b[0m in \u001b[0;36mpreprocess_train_dataframe\u001b[0;34m(total_train_images)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B27.96.130.197/opt/ml/code/train.ipynb#ch0000018vscode-remote?line=21'>22</a>\u001b[0m path_split \u001b[39m=\u001b[39m path\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B27.96.130.197/opt/ml/code/train.ipynb#ch0000018vscode-remote?line=22'>23</a>\u001b[0m id_ \u001b[39m=\u001b[39m path_split[\u001b[39m0\u001b[39m]\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B27.96.130.197/opt/ml/code/train.ipynb#ch0000018vscode-remote?line=23'>24</a>\u001b[0m gender \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mif\u001b[39;00m path_split[\u001b[39m1\u001b[39;49m] \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmale\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B27.96.130.197/opt/ml/code/train.ipynb#ch0000018vscode-remote?line=24'>25</a>\u001b[0m race \u001b[39m=\u001b[39m path_split[\u001b[39m2\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B27.96.130.197/opt/ml/code/train.ipynb#ch0000018vscode-remote?line=25'>26</a>\u001b[0m age \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(path_split[\u001b[39m3\u001b[39m])\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import pandas as pd\n",
    "\n",
    "total_train_images = glob('/opt/ml/input/data/train/images/*/*') # 모든 train 이미지에 대한 경로를 리스트로 받아옵니다.\n",
    "\n",
    "DATA_PATH = '/opt/ml/input/data/train/train.csv'\n",
    "\n",
    "def age_group(x): # 김준재님 코드 참고\n",
    "    return min(2, x // 30)\n",
    "\n",
    "def preprocess_train_dataframe(total_train_images):\n",
    "\n",
    "    new_train_df = pd.DataFrame(columns={\"id\", \"gender\", \"race\", \"age\", \"mask\", \"img_path\", \"path\"})\n",
    "    total_id, total_gender, total_race, total_age, total_img_path, total_mask, total_folder = [], [], [], [], [], [], []\n",
    "\n",
    "    for img_path in total_train_images:\n",
    "\n",
    "        split_list = img_path.split(\"/\")\n",
    "        file_name = split_list[-1]\n",
    "        path = split_list[-2]\n",
    "\n",
    "        path_split = path.split(\"_\")\n",
    "        id_ = path_split[0]\n",
    "        gender = 0 if path_split[1] == \"male\" else 1\n",
    "        race = path_split[2]\n",
    "        age = int(path_split[3])\n",
    "        \n",
    "        if \"normal\" in file_name:\n",
    "            mask = 2\n",
    "        elif \"incorrect\" in file_name:\n",
    "            mask = 1\n",
    "        else:\n",
    "            mask = 0\n",
    "\n",
    "        total_id.append(id_)\n",
    "        total_gender.append(gender)\n",
    "        total_race.append(race)\n",
    "        total_age.append(age)\n",
    "        total_mask.append(mask)\n",
    "        total_img_path.append(img_path)\n",
    "        total_folder.append(path)   \n",
    "\n",
    "    new_train_df['id'] = total_id\n",
    "    new_train_df['gender'] = total_gender\n",
    "    new_train_df['race'] = total_race\n",
    "    new_train_df['age'] = total_age\n",
    "    new_train_df['mask'] = total_mask\n",
    "    new_train_df['img_path'] = total_img_path\n",
    "    new_train_df['path'] = total_folder\n",
    "    \n",
    "    # age group 생성\n",
    "    new_train_df['age_group'] = new_train_df['age'].apply(lambda x : age_group(x))\n",
    "\n",
    "    # label 생성 - 신규범님 코드 참고\n",
    "    new_train_df['label'] = new_train_df['mask'] * 6 + new_train_df['gender']*3 + new_train_df['age_group']\n",
    "    return new_train_df.sort_values(by='id').reset_index(drop=True)\n",
    "\n",
    "new_train_df = preprocess_train_dataframe(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 7))\n",
    "ax = sns.countplot(x=\"label\", data=new_train_df, ax=ax)\n",
    "for p in ax.patches:\n",
    "        ax.annotate('{}'.format(p.get_height()), (p.get_x()+0.15, p.get_height()+30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "14966d3159883f8d917b502f8317f4fd89ea3fcaae61eb59a2333f05d16b4964"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('ImageClassification')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
